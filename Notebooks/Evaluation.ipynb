{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from networks.vgg_lstm import Vgg_Lstm\n",
    "from networks.vgg_bert_custom import vgg_bert\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "# import pypdfium2\n",
    "# import pytesseract\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.models import Doc2Vec\n",
    "from stop_words import get_stop_words\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at GroNLP/bert-base-dutch-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "feature_length = 1024\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model_dbow = Doc2Vec.load(\"../models/gensim/pdf_split_d2v_gensim_{}_db.mod\".format(feature_length))\n",
    "model_dmm = Doc2Vec.load(\"../models/gensim/pdf_split_d2v_gensim_{}_dm.mod\".format(feature_length))\n",
    "vectorizer_gensim = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "\n",
    "model_lstm = Vgg_Lstm(img_dim=3, embedding_dim=2048, output_dim=2)\n",
    "if torch.cuda.is_available():\n",
    "    model_lstm = nn.DataParallel(model_lstm)\n",
    "model_lstm = model_lstm.to(device)\n",
    "weight = torch.load('../models/mlp_model/ce_vgg_lstm_50/best_val_loss.pt', map_location=device)\n",
    "model_lstm.load_state_dict(weight, strict=True)\n",
    "\n",
    "model_bert = vgg_bert(in_ch=3, out_ch=2)\n",
    "if torch.cuda.is_available():\n",
    "    model_bert = nn.DataParallel(model_bert)\n",
    "model_bert = model_bert.to(device)\n",
    "weight = torch.load('../models/mlp_model/focal_bert_newdata_75/best_val_loss.pt', map_location=device)\n",
    "model_bert.load_state_dict(weight, strict=True)\n",
    "\n",
    "model_lstm.eval()\n",
    "model_bert.eval()\n",
    "\n",
    "\n",
    "with open('../models/logreg/logreg_model_1024_dbow_dm_concate.sav', 'rb') as m:\n",
    "    model_log = pickle.load(m)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\", revision=\"v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "def get_bert_tokenizer(text):\n",
    "    bert_input = bert_tokenizer(text, padding='max_length', max_length=256,\n",
    "                               truncation=True, return_tensors=\"pt\")\n",
    "    _id = bert_input['input_ids']\n",
    "    _mask = bert_input['attention_mask']\n",
    "    return _id, _mask\n",
    "\n",
    "def tag_page(prediction):\n",
    "    \"\"\"\n",
    "    :param prediction: classify prediction array: e.g [1, 0, 0, 1, 0, 0, 1, 0, 1]\n",
    "    :return: tag page: e.g. [3, 3, 2, 1]\n",
    "    \"\"\"\n",
    "    tag = np.split(prediction, np.argwhere(prediction == 1).flatten())\n",
    "    tag = [len(tag[i]) for i in range(len(tag)) if len(tag[i])]\n",
    "    tag = np.array(tag)\n",
    "\n",
    "    return tag\n",
    "\n",
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").text\n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text)\n",
    "    text = re.sub(r'\\\\n', r' ', text)\n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('x', '')\n",
    "    return text\n",
    "\n",
    "def resize_normalize_image(image, img_size=224):\n",
    "    height, width, _ = image.shape\n",
    "    if height > width:\n",
    "        scale = img_size / height\n",
    "        resized_height = img_size\n",
    "        resized_width = int(width * scale)\n",
    "    else:\n",
    "        scale = img_size / width\n",
    "        resized_height = int(height * scale)\n",
    "        resized_width = img_size\n",
    "\n",
    "    image = cv2.resize(image, (resized_width, resized_height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    new_image = np.zeros((img_size, img_size, image.shape[2]))\n",
    "\n",
    "    offset_w = (img_size - resized_width) // 2\n",
    "    offset_h = (img_size - resized_height) // 2\n",
    "    new_image[offset_h:offset_h + resized_height, offset_w:offset_w + resized_width] = image\n",
    "    return new_image/255.0\n",
    "\n",
    "\n",
    "def data_loader(df):\n",
    "    df.fillna('', inplace=True)\n",
    "    df['text_processed'] = df['text'].apply(cleanText)\n",
    "    path = df['repaths'].tolist()\n",
    "    text = df['text_processed'].tolist()\n",
    "    # labels = df['labels']\n",
    "    label = df['labels'].tolist()\n",
    "    return path, text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class Prediction():\n",
    "    def __init__(self, path, text, label, data_pct=\"25\", vectorizer_type='gensim', model_type='vgg_bert'):\n",
    "        self.vectorizer_type = vectorizer_type\n",
    "        self.model_type = model_type\n",
    "        self.path = path\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "        self.data_percentage = data_pct\n",
    "        self.output = []\n",
    "\n",
    "    def run_batch(self):\n",
    "        result = []\n",
    "        labels = []\n",
    "        for i in tqdm(range(len(self.path))):\n",
    "            fname = self.path[i]\n",
    "            image = cv2.imread(fname)\n",
    "            text = self.text[i]\n",
    "            predict = self.run_single(image, text)\n",
    "            result.append(predict)\n",
    "            labels.append(self.label[i])\n",
    "        result = np.array(result)\n",
    "        result = tag_page(result)\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        labels = tag_page(labels)\n",
    "        return result, labels\n",
    "\n",
    "\n",
    "    def run_single(self, image, text):\n",
    "        if self.model_type == 'log':\n",
    "            text = tokenize_text(text)\n",
    "            gensim_vectors = vectorizer_gensim.infer_vector(text)\n",
    "            prediction = model_log.predict([gensim_vectors])\n",
    "            return int(prediction.item())\n",
    "        elif self.model_type == 'vgg_lstm':\n",
    "            text = tokenize_text(text)\n",
    "            gensim_vectors = vectorizer_gensim.infer_vector(text)\n",
    "            gensim_vectors = torch.from_numpy(gensim_vectors)\n",
    "            gensim_vectors = torch.unsqueeze(gensim_vectors, dim=0)\n",
    "            gensim_vectors = torch.unsqueeze(gensim_vectors, dim=0)\n",
    "            gensim_vectors = gensim_vectors.to(device)\n",
    "            \n",
    "            image = resize_normalize_image(image)\n",
    "            image = torch.from_numpy(image).float()\n",
    "            image = torch.unsqueeze(image, dim=0)\n",
    "            image = image.permute(0, 3, 1, 2)\n",
    "            image = image.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                prediction = model_lstm(image, gensim_vectors)\n",
    "                prediction = F.softmax(prediction, dim=1)\n",
    "                prediction = torch.argmax(prediction, dim=1)\n",
    "                \n",
    "            prediction = prediction.cpu().numpy()\n",
    "            return int(prediction.item())\n",
    "        else:\n",
    "            # print(\"using bert!\")\n",
    "            b_id, b_mask = get_bert_tokenizer(text)\n",
    "            b_id = b_id.to(device)\n",
    "            b_mask = b_mask.to(device)\n",
    "            \n",
    "            image = resize_normalize_image(image)\n",
    "            image = torch.from_numpy(image).float()\n",
    "            image = torch.unsqueeze(image, dim=0)\n",
    "            image = image.permute(0, 3, 1, 2)\n",
    "            image = image.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                prediction = model_bert(image, b_id, b_mask)\n",
    "                prediction = F.softmax(prediction, dim=1)\n",
    "                prediction = torch.argmax(prediction, dim=1)\n",
    "                \n",
    "            prediction = prediction.cpu().numpy()\n",
    "            return int(prediction.item())\n",
    "\n",
    "\n",
    "def make_index(split):\n",
    "    '''Turns a doc length vector like [1,2,1,3,3,5] into a dict with pagenumbers as keys and the set of all \n",
    "    pagenumbers in the same document as value.\n",
    "    This thus is an index which gives for every page its cluster.'''\n",
    "    l= sum(split)\n",
    "    pages= list(np.arange(l))\n",
    "    out = defaultdict(set)\n",
    "    for block_length in split:\n",
    "        block= pages[:block_length]\n",
    "        pages= pages[block_length:]\n",
    "        for page in block:\n",
    "            out[page]= set(block)\n",
    "    return out\n",
    "\n",
    "#test\n",
    "# make_index(vb_truth)\n",
    "#tests\n",
    "\n",
    "def Bcubed(truth,pred,return_df=False):\n",
    "    assert sum(truth)==sum(pred)  # same amount of pages\n",
    "    truth,pred = make_index(truth), make_index(pred)\n",
    "    if return_df:\n",
    "        df  ={i:{'size':len(truth[i]),'P':0,'R':0,'F1':0} for i in truth}\n",
    "        for i in truth:\n",
    "            df[i]['P']= len(truth[i] & pred[i])/len(pred[i]) \n",
    "            df[i]['R']= len(truth[i] & pred[i])/len(truth[i])\n",
    "            df[i]['F1']= (2*df[i]['P']*df[i]['R'])/(df[i]['P']+df[i]['R'])\n",
    "        df= pd.DataFrame.from_dict(df, orient='index')\n",
    "        df.index_name='PageNr'\n",
    "        return df\n",
    "    else:\n",
    "        P = []\n",
    "        R = []\n",
    "        F1 = []\n",
    "        for i in truth:\n",
    "            P.append(len(truth[i] & pred[i])/len(pred[i]) )\n",
    "            R.append(len(truth[i] & pred[i])/len(truth[i]))\n",
    "            F1.append((2*P[i]*R[i])/(P[i]+R[i]))\n",
    "        return P, R, F1\n",
    "\n",
    "\n",
    "def MeanBcubed(truth,pred,return_df=False):\n",
    "    assert sum(truth)==sum(pred)  # same amount of pages\n",
    "    if return_df:\n",
    "        return Bcubed(truth,pred).mean()\n",
    "    else:\n",
    "        return np.mean(np.array(Bcubed(truth, pred)[0])), np.mean(np.array(Bcubed(truth, pred)[1])), np.mean(np.array(Bcubed(truth, pred)[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asic/anaconda3/envs/pdf_split/lib/python3.7/site-packages/pandas/core/frame.py:5182: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n",
      "/home/asic/anaconda3/envs/pdf_split/lib/python3.7/site-packages/bs4/__init__.py:439: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/home/asic/anaconda3/envs/pdf_split/lib/python3.7/site-packages/ipykernel_launcher.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 211/211 [00:06<00:00, 30.82it/s]\n",
      "100%|██████████| 2585/2585 [01:23<00:00, 30.89it/s]\n",
      "100%|██████████| 637/637 [00:22<00:00, 27.80it/s]\n",
      "100%|██████████| 258/258 [00:08<00:00, 30.97it/s]\n",
      "100%|██████████| 94/94 [00:02<00:00, 32.47it/s]\n",
      "100%|██████████| 276/276 [00:09<00:00, 30.29it/s]\n",
      "100%|██████████| 189/189 [00:05<00:00, 32.75it/s]\n",
      "100%|██████████| 549/549 [00:18<00:00, 29.32it/s]\n",
      "100%|██████████| 1110/1110 [00:36<00:00, 30.73it/s]\n",
      "100%|██████████| 1371/1371 [01:02<00:00, 21.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96608545 0.41941058 0.47548609]\n"
     ]
    }
   ],
   "source": [
    "path_to_csv = '../data/full_ocred.csv'\n",
    "whole_df = pd.read_csv(path_to_csv)\n",
    "whole_df = whole_df.fillna('')\n",
    "df_list = [whole_df[whole_df['names']==x] for x in list(whole_df['names'].unique())]\n",
    "\n",
    "rs = []\n",
    "for df in df_list[:10]:\n",
    "    path, text, label = data_loader(df)\n",
    "    model_predict = Prediction(path=path, text=text, label=label, model_type='vgg_bert')\n",
    "    prediction, labels = model_predict.run_batch()\n",
    "    rs.append(MeanBcubed(labels, prediction))\n",
    "    # rs.append(list(model_predict.run_batch()))\n",
    "print(np.mean(np.array(rs), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../corpus1/TrainTestSet/Trainset/Doclengths_of_the_individual_docs_TRAIN.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36884/291178094.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../corpus1/TrainTestSet/Trainset/Doclengths_of_the_individual_docs_TRAIN.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtruth_corpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../corpus1/TrainTestSet/Trainset/Doclengths_of_the_individual_docs_TRAIN.json'"
     ]
    }
   ],
   "source": [
    "f= open('../corpus1/TrainTestSet/Trainset/Doclengths_of_the_individual_docs_TRAIN.json')\n",
    "truth_corpus=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'truth_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36884/4039621252.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m D ={pdf: MeanBcubed(truth_corpus[pdf], fixedpage(truth_corpus[pdf],6))\n\u001b[0;32m---> 10\u001b[0;31m    for pdf in truth_corpus}\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'truth_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "def fixedpage(truth,docsize=3):\n",
    "    number_of_blocks= sum(truth)//docsize\n",
    "    rest = sum(truth) % docsize\n",
    "    if rest !=0:\n",
    "        return [docsize for _ in range(number_of_blocks)]+[rest]\n",
    "    else:\n",
    "        return [docsize for _ in range(number_of_blocks)]  \n",
    "    \n",
    "D ={pdf: MeanBcubed(truth_corpus[pdf], fixedpage(truth_corpus[pdf],6))\n",
    "   for pdf in truth_corpus}\n",
    "results= pd.DataFrame.from_dict(D,orient='index')\n",
    "print(results.describe())\n",
    "sns.boxplot(data=results[['P','R','F1']]);    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title='Mean Bcubed P,R,F1 for each document in the corpus1 Train set with fixed median (6) doc. size.'\n",
    "results[['P','R','F1']].sort_values('R').reset_index().plot( title=title,\n",
    "                                               figsize=(20,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot.scatter(x='R', y='P', figsize=(10,8));"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "258a5d76ef2b9f7f35f76a4dd70b01132764364364314eda173d4432b108697f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pdf_split')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
