{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhHxhBjJtaXg"
   },
   "source": [
    "## TestTemplate Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znW6yTXstnHS"
   },
   "source": [
    "This notebook should be used to submit your model for testing on the test set and the submission of you model to the general leaderboard. Please copy it to your own submission folder and fill it in. Please note that it is important that you pip install any dependencies that your model needs so that we can easily run the model. In some cases, you might want to upload an already trained model to be evaluated, instead of training your model from scracth. This is HIGHLY recommended if you use models that take a long time to train, but for small sklearn models it is not very necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "\n",
    "If you want to submit your model to be tested on the secret test sets, please implement your model in this 'SubmitRun' notebook, and make sure it works on the sample data. It is easiest to just download the 'Competition' folder for this and check that everything works. If so, please put your notebook and any other files you need (like trained model files). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1652279988171,
     "user": {
      "displayName": "Ruben van Heusden",
      "userId": "02292024052020352966"
     },
     "user_tz": -120
    },
    "id": "NROs7EuDJGRv"
   },
   "outputs": [],
   "source": [
    "# Please install any packages you need in this cell\n",
    "# For example: !pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1652281282360,
     "user": {
      "displayName": "Ruben van Heusden",
      "userId": "02292024052020352966"
     },
     "user_tz": -120
    },
    "id": "Vu7IWWFQt2yn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Metric import\n",
    "import metricutils\n",
    "\n",
    "# import any packages your code might need here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyPVJ984hN_f"
   },
   "source": [
    "You will not have access to the test folder with the test data from both corpora, but you can test that your model will run properly with some samples that we have provided for you in the 'sample_data' directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting paths\n",
    "\n",
    "Depending on where you put hour notebook relative to the data, you might have slighly different paths to the training and sample data. To ensure the code works, please adjust the path in the cell below to point to the 'Competition' folder. In this case of this notebook, the path is just one folder up, so '../'. You can use either absolute or relative paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_competition_folder = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1652281449524,
     "user": {
      "displayName": "Ruben van Heusden",
      "userId": "02292024052020352966"
     },
     "user_tz": -120
    },
    "id": "a5KocCpLkrlv"
   },
   "outputs": [],
   "source": [
    "# This function can be used to load the pdf file names, gold standard json and the text dataframe\n",
    "\n",
    "def get_data(data_path, train=True):\n",
    "    \"\"\"\n",
    "    This function takes as input the path to either train_data or test_data, and combines\n",
    "    the information present in the corpus1 and corpus2 subfolders to allow\n",
    "    you to train on the all the data. the output is a dict containing the dataframe with the text,\n",
    "    and a 'png' column to the path of the png files belonging to each file. It also contains the gold standard in\n",
    "    binary vector format.\n",
    "    \n",
    "    \"\"\"\n",
    "    if train:\n",
    "        gold_standard_path = '%s/Doclengths_of_the_individual_docs_TRAIN.json' % data_path\n",
    "    else:\n",
    "        gold_standard_path = '%s/Doclengths_of_the_individual_docs_TEST.json' % data_path       \n",
    "    \n",
    "    dataframe = pd.read_csv('%s/ocred_text.csv.gz' % data_path)\n",
    "    with open(gold_standard_path, 'r') as json_file:\n",
    "        gold_json = json.load(json_file)\n",
    "    \n",
    "    # Add the png column\n",
    "    png_column = dataframe['name'] + '-' + dataframe['page'].astype(str) + '.png'\n",
    "    \n",
    "    # Make sure it points to the correct path\n",
    "    png_column_joined = os.path.join(data_path, 'png') + os.sep + png_column\n",
    "    dataframe['png'] = png_column_joined\n",
    "    \n",
    "    binary_gold = {key: metricutils.length_list_to_bin(val) for key, val in gold_json.items()}\n",
    "\n",
    "    return {'csv': dataframe.sort_values(by=['name', 'page']), 'json': binary_gold}\n",
    "\n",
    "\n",
    "def get_train_data(data_path):\n",
    "    \"\"\"\n",
    "    This function takes as input the path to 'train_data' and combined\n",
    "    the information present in the corpus1 and corpus2 subfolders to allow\n",
    "    you to train on the all the data.\n",
    "    \"\"\"\n",
    "  # We train on both corpus 1 and corpus 2\n",
    "    c1_path = os.path.join(data_path, 'corpus1')\n",
    "    c2_path = os.path.join(data_path, 'corpus2')\n",
    "        \n",
    "        \n",
    "    c1_data = get_data(c1_path, train=True)\n",
    "    c2_data = get_data(c2_path, train=True)\n",
    "\n",
    "    combined_dataframe = pd.concat([c1_data['csv'], c2_data['csv']])\n",
    "    combined_json = {**c1_data['json'], **c2_data['json']}\n",
    "    combined_pdfs = c1_data['pdf'] + c2_data['pdf']\n",
    "    \n",
    "    return {'csv': combined_dataframe.sort_values(by=['name', 'page']), 'json': combined_json}\n",
    "\n",
    "\n",
    "def get_sample_data(data_path):\n",
    "    \"\"\"\n",
    "    This function takes as input the path to the 'sample_data' folder,\n",
    "    and outputs a dictionary with the text dataframe, the gold standard json and\n",
    "    the paths to the pds.\n",
    "    \"\"\"\n",
    "    dataframe = pd.read_csv('%s/sample.csv' % data_path)\n",
    "    with open('%s/sample.json' % data_path, 'r') as json_file:\n",
    "        gold_json = json.load(json_file)\n",
    "\n",
    "    # Add the png column\n",
    "    png_column = dataframe['name'] + '-' + dataframe['page'].astype(str) + '.png'\n",
    "    \n",
    "    # Make sure it points to the correct path\n",
    "    png_column_joined = os.path.join(data_path, 'png') + os.sep + png_column\n",
    "    dataframe['png'] = png_column_joined\n",
    "        \n",
    "    binary_gold = {key: metricutils.length_list_to_bin(val) for key, val in gold_json.items()}\n",
    "\n",
    "    return {'csv': dataframe.sort_values(by=['name', 'page']), 'json': binary_gold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxrdbP2HvWVK"
   },
   "source": [
    "## Setting up the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5DHEbynvcee"
   },
   "source": [
    "Unless you are loading in a trained model, this is where you want to insert the code to train your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is provided as a dictionary with three entries: `{'csv': _, 'json'}`.csv contains the loaded in csv file, json contains the gold standard json in binary format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcRNpniG6Yfz"
   },
   "source": [
    "We are going to follow an approach that is similar to that of SKLearn, where you make a model that has `train` and `predict` functions. The function to score a modle will be provided by us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2CxWw569yNo"
   },
   "source": [
    "Please also read the `Evaluation` notebook on the surdrive carefully, it shows how the metrics are calculated and what your format should be. Your model should return a dictionary where each key is the document ID and the value is the stream in binary vector format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TBndsqNBqdwc"
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, train_data):\n",
    "        # This should train using the intput dictionary\n",
    "        pass\n",
    "    def predict(self, test_data):\n",
    "        # same input dictionary as in the train function\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model requires trainin you can put this boolean to true. If not, you can leave it as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyModel = Model()\n",
    "\n",
    "from_scratch = False\n",
    "if from_scratch:\n",
    "    train_data = get_train_data(os.path.join(path_to_competition_folder, 'train_data'))\n",
    "    MyModel.train(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9DqXlYQr3Aw"
   },
   "source": [
    "## Checking the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we will make a prediction with the model on the sample data, and see how whether it works and gives us some scores. Please also add a descriptive name of your model below (preferably also containing your name), this will be used as a title of the plots shown on the test sets, and helps us keep it clear which plots came from which models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_model_description = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3pfWdyOxr42g"
   },
   "outputs": [],
   "source": [
    "run_on_test = False\n",
    "\n",
    "if run_on_test:\n",
    "    \n",
    "    corpus1_test_data = get_data(os.path.join(path_to_competition_folder, 'test_data/corpus1'), train=False)\n",
    "    corpus2_test_data = get_data(os.path.join(path_to_competition_folder, 'test_data/corpus2'), train=False)\n",
    "    \n",
    "    predictions_corpus1 = predictions = MyModel.predict(corpus1_test_data)\n",
    "    predictions_corpus2 = predictions = MyModel.predict(corpus2_test_data)\n",
    "\n",
    "else:\n",
    "    \n",
    "    data = get_sample_data(os.path.join(path_to_competition_folder, 'sample_data'))\n",
    "    \n",
    "    # You should not have to adjust anything here, the model should just return predections in binary format\n",
    "    predictions = MyModel.predict(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_on_test:\n",
    "    display(Markdown(\"<b>Scores for the model on corpus 1</b>\"))\n",
    "    metricutils.evaluation_report(corpus1_test_data['json'], predictions_corpus1, title=short_model_description)\n",
    "    \n",
    "    display(Markdown(\"<b>Scores for the model on corpus 2</b>\"))\n",
    "    metricutils.evaluation_report(corpus2_test_data['json'], predictions_corpus2, title=short_model_description)\n",
    "else:\n",
    "    display(Markdown(\"<b>Scores for the model on the sample data</b>\"))\n",
    "    metricutils.evaluation_report(data['json'], predictions, title=short_model_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO31tkava5l0E+vsk+jHNvg",
   "collapsed_sections": [],
   "mount_file_id": "1EYIZemYSfPdd-ieKytv5jPwxRenXbfqn",
   "name": "SubmitRun.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
