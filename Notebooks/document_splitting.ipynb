{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tqdm\n",
    "import json\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata is saved in 2 files (for 2 corpuses) which has the following structure:\n",
    "```\n",
    "{\n",
    "    \"893872\": [1, 168, 13, 1, 12, 2, 3, 26, 1, 6], \n",
    "    \"890952\": [151],...\n",
    "}\n",
    "```\n",
    "The keys would be the name of documents and the values would be the number of pages of each sub-documents. We call this **gold** labels.\n",
    "Having this **gold** labels, we need to convert to **one hot** vectors for further training and to save each page of sub documents to appropriate folder (1.0 for first page, 0.0 for not first pages)\n",
    "\n",
    "The function ```gold_to_onehot``` below is used to convert the **gold** to **one hot**. The rule is as follow:\n",
    "\n",
    "say we have gold like this ```[4,5,1]```, meaning that the document has 10 pages, containing 3 sub-documents that has 4, 5, and 1 page, respectively.\n",
    "We will convert to one hot as follow:\n",
    "```[4, 5, 1] -> [1,0,0,0,1,0,0,0,0,1]```\n",
    "\n",
    "Specifically, first sub-doc has 4 pages, so its **one hot** will be ```1,0,0,0``` (1 for the first page, 0 for others)\n",
    "same for 2 others.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_doc_meta(json_file):\n",
    "    with open(json_file, 'rb') as f:\n",
    "        meta = json.load(f)\n",
    "    return meta\n",
    "\n",
    "def gold_to_onehot(original_indexes):\n",
    "    all = np.array([])\n",
    "    for i in original_indexes:\n",
    "        temp = np.concatenate((np.ones((1)), np.zeros((i-1))))\n",
    "        all = np.concatenate((all, temp))\n",
    "    return list(all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the **one hot** labels, we can use that for extracting every single page and save it to appropriate folder (1.0 for first pages, 0.0 for the others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(df):\n",
    "    files = list(df[\"names\"])\n",
    "    labels = list(df[\"one_hot\"])\n",
    "    for f, label in tqdm.tqdm(zip(files, labels)):\n",
    "        if len(label) >= 500:\n",
    "            continue\n",
    "        name = f.split('/')[-1][:-4]\n",
    "        images = convert_from_path(f)\n",
    "        image_count = 0\n",
    "        for image, l in zip(images, label):\n",
    "            filename = \"images/{}/{}_p_{}.jpg\".format(str(l), name, str(image_count))\n",
    "            image.save(filename, 'JPEG')\n",
    "            image_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can extract text in each image using pytesseract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(path):\n",
    "    filename = path\n",
    "    text = str(((pytesseract.image_to_string(Image.open(filename)))))\n",
    "    text = text.replace('-\\n', '')  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main program starts here\n",
    "1. First we read the meta json file, extract one hot labels\n",
    "2. Save the *name*, *pages* (original gold), and *one hot* to dataframes\n",
    "3. Extract images\n",
    "4. Extract text and save to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus1/TrainTestSet/Trainset/Doclengths_of_the_individual_docs_TRAIN.json', 'rb') as f:\n",
    "    meta = json.load(f)\n",
    "names = ['corpus1/TrainTestSet/Trainset/data/' + name + \"__concatenated.pdf\" for name in meta.keys()]\n",
    "pages = meta.values()\n",
    "df_cp1 = pd.DataFrame({\"names\":names, \"pages\":pages})\n",
    "df_cp1[\"one_hot\"] = df_cp1[\"pages\"].apply(gold_to_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus2/TrainTestSet/Trainset/Doclengths_of_the_individual_docs_TRAIN.json', 'rb') as f:\n",
    "    meta = json.load(f)\n",
    "names = ['corpus2/TrainTestSet/Trainset/data/' + name + \"__concatenated.pdf\" for name in meta.keys()]\n",
    "pages = meta.values()\n",
    "df_cp2 = pd.DataFrame({\"names\":names, \"pages\":pages})\n",
    "df_cp2[\"one_hot\"] = df_cp2[\"pages\"].apply(gold_to_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract images\n",
    "df = df_cp1.append(df_cp2)\n",
    "extract_images(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find list of images and its labels\n",
    "data_list_0 = glob.glob('images/images/0.0/*.jpg')\n",
    "data_list_1 = glob.glob('images/images/1.0/*.jpg')\n",
    "data_list = data_list_0 + data_list_1\n",
    "labels = list(np.concatenate((np.zeros((len(data_list_0))), np.ones((len(data_list_1))))))\n",
    "df = pd.DataFrame({\"paths\": data_list, \"labels\": labels})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too many images, we need parallel processing\n",
    "tqdm.tqdm.pandas()\n",
    "from pandarallel import pandarallel\n",
    "from pqdm.processes import pqdm\n",
    "\n",
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that we save the extracted images for \"1.0\" images and \"0.0\" images in separate csv files so we can use it for training more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_1 = np.ones((len(data_list_1)))\n",
    "df_1 = pd.DataFrame({\"paths\": data_list_1, \"labels\": labels_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text \n",
    "df_1[\"text\"] = df_1[\"paths\"][:].parallel_apply(extract_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_0 = np.ones((len(data_list_0)))\n",
    "df_0 = pd.DataFrame({\"paths\": data_list_0, \"labels\": labels_0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0[\"text\"] = df_0[\"paths\"][:].parallel_apply(extract_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv files\n",
    "df_0.to_csv('data/0.0.csv', index=False)\n",
    "df_1.to_csv('data/1.0.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c6db50b4d1f5928cfc9fdd61658a551a4e1af316797848a4d87ad9efbdb0ec0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
