{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d14684af",
   "metadata": {},
   "source": [
    "# Bcubed precision and recall\n",
    "\n",
    "* Definition in *Amig√≥ E, Gonzalo J, Artiles J et alA comparison of extrinsic clustering evaluation metrics based on formal constraints.   Information Retrieval. 12. 461-486. 10.1007/s10791-008-9066-8.*\n",
    "\n",
    "![](BCPR.png)\n",
    "\n",
    "\n",
    "1. On the web you can quickly find definitions of bcubed precision and recall: see eg <https://www.researchgate.net/figure/Example-of-computing-the-BCubed-precision-and-recall-for-one-item_fig10_225548032> for a nice picture.\n",
    "2. Your function  will have as input two vectors: true and predicted. \n",
    "3. The sum of the values in that list equals the numbefr of pages of the PDF file.\n",
    "4. The  length of that list is the number of documents in the PDF file\n",
    "5. The numbers in the list indicate the lengths of the documents. So L[i] gives the length in number of pages of the i-th document in the PDF file.\n",
    "6. You find an example on `DocumentSplitting/data/WobGelderland/Doclengths_of_the_individual_docs.json` for the files in `/DocumentSplitting/data/WobGelderland/ConcatenatedPDFS`  \n",
    " \n",
    "\n",
    "### baselines\n",
    "\n",
    "* every page starts a new doc ([1,1,1,1,......])\n",
    "* there is only one doc  ([X]) for X the length of the compolete document\n",
    "* but also more informed ones, like the mode and the median of the true splits \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef52fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8711976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from networks.vgg_lstm import Vgg_Lstm\n",
    "# from networks.vgg_bert_custom import vgg_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21013753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "# import pypdfium2\n",
    "# import pytesseract\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.models import Doc2Vec\n",
    "from stop_words import get_stop_words\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3a58ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.0.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "feature_length = 1024\n",
    "model_dbow = Doc2Vec.load(\"gensim/pdf_split_d2v_gensim_{}_db.mod\".format(feature_length))\n",
    "model_dmm = Doc2Vec.load(\"gensim/pdf_split_d2v_gensim_{}_dm.mod\".format(feature_length))\n",
    "vectorizer_gensim = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "model_lstm = Vgg_Lstm(img_dim=3, embedding_dim=2048, output_dim=2)\n",
    "# model_bert = vgg_bert(in_ch=3, out_ch=2)\n",
    "with open('models/logregmodel_lstmlogreg_model_1024_dbow_dm_concate_100.sav', 'rb') as m:\n",
    "    model_log = pickle.load(m)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "def tag_page(prediction):\n",
    "    \"\"\"\n",
    "    :param prediction: classify prediction array: e.g [1, 0, 0, 1, 0, 0, 1, 0, 1]\n",
    "    :return: tag page: e.g. [3, 3, 2, 1]\n",
    "    \"\"\"\n",
    "    tag = np.split(prediction, np.argwhere(prediction == 1).flatten())\n",
    "    tag = [len(tag[i]) for i in range(len(tag)) if len(tag[i])]\n",
    "    tag = np.array(tag)\n",
    "\n",
    "    return tag\n",
    "\n",
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").text\n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text)\n",
    "    text = re.sub(r'\\\\n', r' ', text)\n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('x', '')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6770e293",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../sample_data/sample_f.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-49e15bb93542>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../sample_data/sample_f.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../sample_data/sample_f.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../sample_data/sample_f.csv')\n",
    "df['name'].unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1908bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(df):\n",
    "    df.fillna('', inplace=True)\n",
    "    df['text_processed'] = df['text'].apply(cleanText)\n",
    "    path = df['fname'].tolist()\n",
    "    text = df['text_processed'].tolist()\n",
    "    # label = df['labels'].tolist()\n",
    "    return path, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7383d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction():\n",
    "    def __init__(self, path, text, data_pct=\"25\", vectorizer_type='gensim', model_type='log'):\n",
    "        self.vectorizer_type = vectorizer_type\n",
    "        self.model_type = model_type\n",
    "        self.path = path\n",
    "        self.text = text\n",
    "        self.data_percentage = data_pct\n",
    "        self.output = []\n",
    "\n",
    "    def run_batch(self):\n",
    "        result = []\n",
    "        for i in range(len(self.path)):\n",
    "            fname = self.path[i]\n",
    "            image = cv2.imread(fname)\n",
    "            text = self.text[i]\n",
    "            predict = self.run_single(image, text)\n",
    "            result.append(predict)\n",
    "        result = np.array(result)\n",
    "        result = tag_page(result)\n",
    "        return result\n",
    "\n",
    "\n",
    "    def run_single(self, image, text):\n",
    "        if self.model_type == 'log':\n",
    "            text = tokenize_text(text)\n",
    "            gensim_vectors = vectorizer_gensim.infer_vector(text)\n",
    "            prediction = model_log.predict([gensim_vectors])\n",
    "            return int(prediction.item())\n",
    "        elif self.model_type == 'vgg_lstm':\n",
    "            text = tokenize_text(text)\n",
    "            gensim_vectors = vectorizer_gensim.infer_vector(text)\n",
    "            ...\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ccd32ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/f9jwgyf91j53cx39tj4fmv700000gp/T/ipykernel_24551/907674630.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.fillna('', inplace=True)\n",
      "/var/folders/k7/f9jwgyf91j53cx39tj4fmv700000gp/T/ipykernel_24551/907674630.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_processed'] = df['text'].apply(cleanText)\n",
      "/var/folders/k7/f9jwgyf91j53cx39tj4fmv700000gp/T/ipykernel_24551/907674630.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.fillna('', inplace=True)\n",
      "/var/folders/k7/f9jwgyf91j53cx39tj4fmv700000gp/T/ipykernel_24551/907674630.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_processed'] = df['text'].apply(cleanText)\n"
     ]
    }
   ],
   "source": [
    "path_to_csv = '../sample_data/sample_f.csv'\n",
    "whole_df = pd.read_csv(path_to_csv)\n",
    "df_list = [whole_df[whole_df['name']==x] for x in list(whole_df['name'].unique())]\n",
    "\n",
    "rs = []\n",
    "for df in df_list:\n",
    "    path, text = data_loader(df)\n",
    "    model_predict = Prediction(path=path, text=text)\n",
    "    rs.append(list(model_predict.run_batch()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c04b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples\n",
    "# vb_truth, vb_pred = [1,2,1,3,3,5], [1,1,2,3,4,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187fbfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_index(split):\n",
    "    '''Turns a doc length vector like [1,2,1,3,3,5] into a dict with pagenumbers as keys and the set of all \n",
    "    pagenumbers in the same document as value.\n",
    "    This thus is an index which gives for every page its cluster.'''\n",
    "    l= sum(split)\n",
    "    pages= list(np.arange(l))\n",
    "    out = defaultdict(set)\n",
    "    for block_length in split:\n",
    "        block= pages[:block_length]\n",
    "        pages= pages[block_length:]\n",
    "        for page in block:\n",
    "            out[page]= set(block)\n",
    "    return out\n",
    "\n",
    "#test\n",
    "make_index(vb_truth)\n",
    "#tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133aa31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bcubed(truth,pred):\n",
    "    assert sum(truth)==sum(pred)  # same amount of pages\n",
    "    truth,pred = make_index(truth), make_index(pred)\n",
    "    \n",
    "    df  ={i:{'size':len(truth[i]),'P':0,'R':0,'F1':0} for i in truth}\n",
    "    for i in truth:\n",
    "        df[i]['P']= len(truth[i] & pred[i])/len(pred[i]) \n",
    "        df[i]['R']= len(truth[i] & pred[i])/len(truth[i])\n",
    "        df[i]['F1']= (2*df[i]['P']*df[i]['R'])/(df[i]['P']+df[i]['R'])\n",
    "    df= pd.DataFrame.from_dict(df, orient='index')\n",
    "    df.index_name='PageNr'\n",
    "    return  df\n",
    "\n",
    "\n",
    "def MeanBcubed(truth,pred):\n",
    "    assert sum(truth)==sum(pred)  # same amount of pages\n",
    "    return Bcubed(truth,pred).mean()\n",
    "\n",
    "\n",
    "print(MeanBcubed(vb_truth,vb_pred))\n",
    "\n",
    "Bcubed(vb_truth,vb_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5698eeca",
   "metadata": {},
   "source": [
    "# On corpus\n",
    "\n",
    "* baseline is fixed cluster/document size with all the median pagelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2102ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open('../corpus1/TrainTestSet/Trainset/Doclengths_of_the_individual_docs_TRAIN.json')\n",
    "truth_corpus=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e68db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixedpage(truth,docsize=3):\n",
    "    number_of_blocks= sum(truth)//docsize\n",
    "    rest = sum(truth) % docsize\n",
    "    if rest !=0:\n",
    "        return [docsize for _ in range(number_of_blocks)]+[rest]\n",
    "    else:\n",
    "        return [docsize for _ in range(number_of_blocks)]  \n",
    "    \n",
    "D ={pdf: MeanBcubed(truth_corpus[pdf], fixedpage(truth_corpus[pdf],6))\n",
    "   for pdf in truth_corpus}\n",
    "results= pd.DataFrame.from_dict(D,orient='index')\n",
    "print(results.describe())\n",
    "sns.boxplot(data=results[['P','R','F1']]);    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8944d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "title='Mean Bcubed P,R,F1 for each document in the corpus1 Train set with fixed median (6) doc. size.'\n",
    "results[['P','R','F1']].sort_values('R').reset_index().plot( title=title,\n",
    "                                               figsize=(20,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot.scatter(x='R', y='P', figsize=(10,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc6aa47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bcb954143aee65b8cdd288c3318a9bb351ad3fa18f9778dd8cac692b1a24c7fc"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
